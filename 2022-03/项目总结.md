## Jupyter
- 项目难点
主要难点在系统设计上，涉及的上下游服务非常多，而且整个系统是从0到1搭建的

首先我先说说我们的系统架构以及服务调用关系

其中有个Hub服务，Hub服务主要负责管理所有用户SSO登录，鉴权以及容器启停，包括将用户界面上的请求通过代理转发到具体的容器端口

用户进入Hub界面后，可以选择对应镜像和队列资源进行启动，这里容器的镜像是由我们构建的，我们构建了一套CI流程，提交代码后通过Jenkins自动化构建镜像、测试、发布的流程

用户可以选择自己需要的镜像进行启动，启动后，将用户url添加到代理层，后续用户的请求会路由到用户容器内

用户容器内会有个notebook server进程，这个进程通过websocket和界面交互，用户执行代码，会首先由websocket从前端发到notebook server，
notebook server通过zmq消息队列将执行信息发到具体的执行内核端（比如说Python内核），内核端执行完成会将执行结果通过zmq再发回给server端，
server端通过websocket发送给前端，前端界面渲染。这个是用户简单执行的流程。

用户还有例行化生产的需求，每天处理指定当天分区的数据，做例行化的数据处理和训练。我们也开发了一套对应的调度生产，根据用户的配置定期拉起容器执行用户代码

还有一些难点就是，经常会遇到一些棘手难排查的问题，比如内核卡住、内核开多了卡顿的一系列问题，我也一一进行了排查，最终都解决了


以现在的角度看过去，就感觉还好，不过可以以当时的角度讲一下，以Jupyter这个项目为例，当时遇到比较棘手的几个点
1. 和传统的后端服务不太一样，传统的后端服务技术栈和框架都比较确定，Jupyter这个项目涉及的组件非常丰富，我简单讲一下Jupyter的基本架构和内核通信的基本原理，
首先我们有一个Jupyter Hub服务，它主要负责多用户管理，认证以及用户扩缩容等，相当于一个中心管理节点，它还包含了一个反向代理层，用户在前端执行一段代码会通过websocket
把请求信息打到代理层，代理层通过请求uri将请求转发到对应用户容器进程端口上，这个进程就是Jupyter Server进程，收到对应请求信息后经过一系列处理它会通过ZMQ将信息
发送给执行内核，执行内核可以是Python、Scala等，最后执行内核执行完成后会将结果通过ZMQ返回，然后通过websocket返回给前端，前端渲染

在这个过程中，会有很多可能出现问题的地方，比如第一个是我们的中心管理节点一开始使用的是开源方案，它有状态的单点服务，内存中缓存有大量的状态信息 无法横向扩展
并且反向代理层也耦合在Jupyter Hub也就是中心管理节点中。服务发布时会影响代理层从而影响所有用户的操作。
第二个是会遇到很多很难复现的问题，比如之前经常遇到内核假死之类的问题，

2. 

## Serving
大组内分工，主要分为三层，

平台产品层 -> 资源调度层 -> 引擎层

平台产品层主要是面向业务的各个产品化方案，主要有样本构建，特征生产管理，模型训练，模型预测几个方向的

资源调度层主要是负责集群资源的调度，主要包含yarn集群和k8s集群

引擎层包含训练引擎和预测引擎层，分别为模型训练和模型预测两个场景的引擎提供支持

这里主要讲模型预测服务，它的核心功能，简单来说就可以在上面把训练好的模型部署在集群的机器上，对外提供预测服务

功能上，提供了 模型管理，模型跨集群同步，模型优化，在线服务部署和服务扩缩容

难点：
1. 对于我个人来说，
2. 其他的，在这种大规模分布式系统中，一些小的细节就可以造成比较严重的case，

主要的变动
1. 核心的流程，幂等的流程均增加了超时以及失败重试的策略
2. 做一些状态的异常检测，因为跨多个系统，且不是强一致模型，较大概率可能出现状态不一致的情况，针对这种情况做告警处理，以及最终一致的处理
3. 模型更新发布流程的优化，之前的不能很好控制发布粒度以及并行度等，针对这个做改造
4. 分流隔离限流，分而治之，不同业务通过header中的信息，请求不同set，每个set对应一组server
